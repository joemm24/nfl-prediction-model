# ðŸ—ï¸ NFL Prediction Model - System Architecture

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Sources                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  nflfastR/   â”‚  â”‚  Play-by-    â”‚  â”‚    Team      â”‚          â”‚
â”‚  â”‚  NFLVerse    â”‚  â”‚  Play Data   â”‚  â”‚  Metadata    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Ingestion Layer                          â”‚
â”‚                     (fetch_data.py)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Fetch schedules, stats, rosters, PBP data            â”‚  â”‚
â”‚  â”‚  â€¢ Clean and standardize team names                     â”‚  â”‚
â”‚  â”‚  â€¢ Save to data/raw/ (CSV/Parquet)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Feature Engineering Layer                       â”‚
â”‚                   (build_features.py)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Rolling averages (points, wins, form)                â”‚  â”‚
â”‚  â”‚  â€¢ Team differentials (home vs. away)                   â”‚  â”‚
â”‚  â”‚  â€¢ Advanced metrics (EPA, success rate)                 â”‚  â”‚
â”‚  â”‚  â€¢ Home/away splits                                     â”‚  â”‚
â”‚  â”‚  â€¢ 40+ engineered features                              â”‚  â”‚
â”‚  â”‚  â€¢ Save to data/features/ (CSV)                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Training Layer                          â”‚
â”‚                       (train.py)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Models:                                                 â”‚  â”‚
â”‚  â”‚  1. Logistic Regression (baseline)                      â”‚  â”‚
â”‚  â”‚  2. Random Forest (ensemble)                            â”‚  â”‚
â”‚  â”‚  3. XGBoost (gradient boosting)                         â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Evaluation:                                             â”‚  â”‚
â”‚  â”‚  â€¢ Cross-validation (5-fold)                            â”‚  â”‚
â”‚  â”‚  â€¢ Metrics: Accuracy, ROC-AUC, Log Loss                 â”‚  â”‚
â”‚  â”‚  â€¢ Feature importance analysis                          â”‚  â”‚
â”‚  â”‚  â€¢ Best model selection                                 â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Output: models/*.pkl, metrics.json                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Prediction Layer                               â”‚
â”‚                     (predict.py)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Load best model                                       â”‚  â”‚
â”‚  â”‚  â€¢ Generate win probabilities                           â”‚  â”‚
â”‚  â”‚  â€¢ Calculate confidence scores                          â”‚  â”‚
â”‚  â”‚  â€¢ Export to predictions/ (JSON/CSV)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                        â”‚
               â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard (Streamlit)  â”‚  â”‚      API (Flask)         â”‚
â”‚     (dashboard.py)       â”‚  â”‚       (api.py)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Interactive UI        â”‚  â”‚  Endpoints:              â”‚
â”‚  â€¢ Matchup cards         â”‚  â”‚  â€¢ GET /predict          â”‚
â”‚  â€¢ Probability charts    â”‚  â”‚  â€¢ GET /predict/matchup  â”‚
â”‚  â€¢ Model metrics         â”‚  â”‚  â€¢ GET /metrics          â”‚
â”‚  â€¢ CSV export            â”‚  â”‚  â€¢ GET /teams            â”‚
â”‚  â€¢ Filters & analysis    â”‚  â”‚  â€¢ GET /health           â”‚
â”‚                          â”‚  â”‚                          â”‚
â”‚  Port: 8501              â”‚  â”‚  Port: 5000              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                        â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   End Users     â”‚
                  â”‚  â€¢ Analysts     â”‚
                  â”‚  â€¢ Applications â”‚
                  â”‚  â€¢ Dashboards   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### 1. Data Ingestion
```
nflfastR API â†’ fetch_data.py â†’ data/raw/
                                  â”œâ”€â”€ schedules.csv
                                  â”œâ”€â”€ team_stats.csv
                                  â”œâ”€â”€ teams.csv
                                  â”œâ”€â”€ rosters.csv
                                  â””â”€â”€ pbp_data.parquet
```

### 2. Feature Engineering
```
data/raw/ â†’ build_features.py â†’ data/features/
                                   â”œâ”€â”€ features.csv (training)
                                   â””â”€â”€ features_all.csv (all games)
```

### 3. Model Training
```
data/features/features.csv â†’ train.py â†’ models/
                                          â”œâ”€â”€ logistic_regression_latest.pkl
                                          â”œâ”€â”€ random_forest_latest.pkl
                                          â”œâ”€â”€ xgboost_latest.pkl
                                          â”œâ”€â”€ best_model_latest.pkl
                                          â”œâ”€â”€ metrics.json
                                          â””â”€â”€ feature_importance.csv
```

### 4. Prediction Generation
```
models/best_model_latest.pkl + data/features/features_all.csv
    â†’ predict.py â†’ predictions/
                     â”œâ”€â”€ predictions_latest.json
                     â””â”€â”€ predictions_latest.csv
```

### 5. Consumption
```
predictions/ â†’ dashboard.py (Streamlit UI)
            â†’ api.py (REST API)
```

## Component Details

### Core Modules

#### 1. utils.py
**Purpose**: Shared utility functions

**Key Functions**:
- `load_config()` - Load YAML configuration
- `save_model()` / `load_model()` - Model persistence
- `save_predictions()` - Export predictions
- `calculate_rolling_stats()` - Rolling averages
- `get_feature_importance()` - Feature analysis

#### 2. fetch_data.py
**Purpose**: Data acquisition from NFL sources

**Key Classes**:
- `NFLDataFetcher` - Main data fetching class

**Methods**:
- `fetch_schedules()` - Game schedules
- `fetch_team_stats()` - Team statistics
- `fetch_pbp_stats()` - Play-by-play data
- `fetch_rosters()` - Player rosters
- `fetch_all_data()` - Complete data pipeline

#### 3. build_features.py
**Purpose**: Transform raw data into ML features

**Key Classes**:
- `NFLFeatureBuilder` - Feature engineering pipeline

**Methods**:
- `prepare_game_results()` - Clean game data
- `calculate_rolling_team_stats()` - Rolling averages
- `calculate_advanced_metrics()` - EPA, success rate
- `merge_game_features()` - Combine all features

#### 4. train.py
**Purpose**: Train and evaluate ML models

**Key Classes**:
- `NFLModelTrainer` - Model training pipeline

**Methods**:
- `train_logistic_regression()` - Baseline model
- `train_random_forest()` - Ensemble model
- `train_xgboost()` - Gradient boosting
- `evaluate_model()` - Performance metrics
- `cross_validate_model()` - CV evaluation

#### 5. predict.py
**Purpose**: Generate predictions for games

**Key Classes**:
- `NFLPredictor` - Prediction generation

**Methods**:
- `load_best_model()` - Load trained model
- `generate_predictions()` - Predict outcomes
- `predict_matchup()` - Single game prediction
- `display_predictions()` - Format output

#### 6. dashboard.py
**Purpose**: Interactive web dashboard

**Key Classes**:
- `NFLDashboard` - Streamlit application

**Features**:
- Matchup cards with probabilities
- Interactive filters
- Model performance metrics
- Prediction analysis charts
- CSV download

#### 7. api.py
**Purpose**: REST API for programmatic access

**Endpoints**:
- `GET /` - API info
- `GET /health` - Health check
- `GET /predict` - Weekly predictions
- `GET /predict/matchup` - Specific game
- `GET /metrics` - Model performance
- `GET /teams` - Team list

## Configuration System

### config.yaml Structure

```yaml
data:              # Data settings
  raw_dir: "data/raw"
  start_season: 2010
  end_season: 2024

features:          # Feature engineering
  rolling_window: 4
  min_games: 3

model:             # Model configuration
  target: "home_team_win"
  test_size: 0.2
  models: [logistic_regression, random_forest, xgboost]

hyperparameters:   # Model-specific params
  xgboost:
    n_estimators: 200
    max_depth: 6
    learning_rate: 0.1

predictions:       # Prediction settings
  output_dir: "predictions"
  export_formats: [json, csv]

api:              # API configuration
  host: "0.0.0.0"
  port: 5000

dashboard:        # Dashboard settings
  port: 8501
```

## Execution Flow

### Full Pipeline

```
1. Fetch Data
   â””â”€> fetch_data.py (15-30 min)
       â””â”€> data/raw/

2. Build Features
   â””â”€> build_features.py (2-5 min)
       â””â”€> data/features/

3. Train Models
   â””â”€> train.py (5-10 min)
       â””â”€> models/

4. Generate Predictions
   â””â”€> predict.py (< 1 min)
       â””â”€> predictions/

5. Serve Results
   â”œâ”€> dashboard.py (interactive UI)
   â””â”€> api.py (REST API)
```

### Command Line Interface

```bash
# Automated pipeline
python run_pipeline.py --full

# Individual steps
python run_pipeline.py --fetch
python run_pipeline.py --features
python run_pipeline.py --train
python run_pipeline.py --predict --season 2024 --week 5

# Launch services
streamlit run src/dashboard.py  # Dashboard on :8501
python src/api.py               # API on :5000
```

## Scalability Considerations

### Current Implementation
- **Data Storage**: Local CSV/Parquet files
- **Model Storage**: Local pickle files
- **Compute**: Single machine
- **Deployment**: Local or single server

### Future Enhancements
- **Data Storage**: PostgreSQL or cloud storage (S3)
- **Model Storage**: MLflow or model registry
- **Compute**: Distributed training (Dask, Ray)
- **Deployment**: Kubernetes, Docker containers
- **Caching**: Redis for API responses
- **Queue**: Celery for async tasks

## Security

### Current
- Optional API key authentication
- CORS enabled for cross-origin requests
- Input validation on API endpoints

### Production Recommendations
- Enable API authentication
- Rate limiting
- HTTPS/TLS encryption
- Database credentials in secrets
- Regular dependency updates

## Monitoring & Logging

### Current Implementation
- Python logging throughout all modules
- Model metrics tracked in metrics.json
- Console output for pipeline progress

### Production Recommendations
- Centralized logging (ELK stack)
- Application performance monitoring (APM)
- Model drift detection
- Alerting on failures
- Performance dashboards

## Technology Stack Summary

| Layer | Technologies |
|-------|-------------|
| Language | Python 3.11+ |
| Data Processing | pandas, numpy |
| Data Source | nfl-data-py (nflfastR) |
| ML Framework | scikit-learn |
| ML Models | Logistic Regression, Random Forest, XGBoost |
| Web Framework | Flask (API), Streamlit (Dashboard) |
| Visualization | Plotly, matplotlib, seaborn |
| Storage | CSV, Parquet, SQLite |
| Configuration | YAML |
| Version Control | Git |

## Performance Characteristics

### Data Pipeline
- **Fetch**: 15-30 minutes (depends on seasons)
- **Features**: 2-5 minutes
- **Training**: 5-10 minutes
- **Predictions**: < 1 minute

### API
- **Response Time**: < 100ms (cached predictions)
- **Throughput**: 100+ requests/sec (single instance)

### Dashboard
- **Load Time**: < 3 seconds
- **Interactivity**: Real-time filtering and updates

## Development Workflow

```
1. Code Changes
   â””â”€> Edit src/*.py

2. Testing
   â”œâ”€> Run linter: flake8 src/
   â”œâ”€> Test pipeline: python run_pipeline.py --full
   â””â”€> Manual testing: Dashboard & API

3. Commit
   â””â”€> git commit -m "feat: description"

4. Deploy
   â””â”€> Push to production server
```

## Deployment Options

### Option 1: Local Development
```bash
python run_pipeline.py --full
streamlit run src/dashboard.py
python src/api.py
```

### Option 2: Cloud VM (AWS EC2, GCP Compute)
```bash
# Setup
./setup.sh

# Run services with screen/tmux
screen -S dashboard streamlit run src/dashboard.py
screen -S api python src/api.py
```

### Option 3: Docker (Future)
```bash
docker-compose up
```

### Option 4: Kubernetes (Future)
```bash
kubectl apply -f k8s/
```

---

**Architecture Status**: âœ… **PRODUCTION-READY**

Modular, scalable, and maintainable design suitable for both development and production deployment.

